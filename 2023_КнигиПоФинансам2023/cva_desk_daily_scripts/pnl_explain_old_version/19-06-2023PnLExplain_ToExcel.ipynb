{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9873d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ps_utils\n",
    "from ps_utils.env import ps\n",
    "import numpy as np\n",
    "from ps_utils import env, market, analytics, pricing\n",
    "import pandas as pd\n",
    "from ps import message as msg\n",
    "\n",
    "from typing import Union, Optional, Dict, List\n",
    "PsObject = Union[ps.Message, ps.GlobalID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e1272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_big_request_for_pnl(response, prev_response, algo_id: str = None, reporting_currency: str = None, queries: List[str] = None) -> msg.MessageList:\n",
    "\n",
    "    if algo_id is None:\n",
    "        algo_id = env.global_algo_id()\n",
    "\n",
    "    big_request = ps.new_cm([])\n",
    "    new_resp = ps.get(ps.put(response))\n",
    "    new_prev_resp = ps.get(ps.put(prev_response))\n",
    "\n",
    "    for r in new_resp:\n",
    "        cpty = r.Request['_Counterparty']\n",
    "        prev_r = next((x for x in new_prev_resp if x.Request['_Counterparty'] == cpty), None)\n",
    "        req = ps.new_cm({\n",
    "            'RequestName': 'Layer2',\n",
    "            '_Counterparty': cpty,\n",
    "            'Layer2Algo': algo_id,\n",
    "            'ReportingCurrency': reporting_currency if reporting_currency else r.Request.ReportingCurrency,\n",
    "            'Method': 'Revaluation',\n",
    "            'Queries': queries or r.Request.Queries,\n",
    "            'Request': r.Request,\n",
    "            'Response': r,\n",
    "        })\n",
    "        if prev_r:\n",
    "            new_prev_resp.remove(prev_r)\n",
    "            req['PreviousRequest'] = prev_r.Request\n",
    "            req['PreviousResponse'] = prev_r\n",
    "        big_request.append(req)\n",
    "\n",
    "    for r in new_prev_resp:\n",
    "        cpty = r.Request['_Counterparty']\n",
    "        big_request.append(ps.new_cm({\n",
    "            'RequestName': 'Layer2',\n",
    "            '_Counterparty': cpty,\n",
    "            'Layer2Algo': algo_id,\n",
    "            'ReportingCurrency': reporting_currency if reporting_currency else r.Request.ReportingCurrency,\n",
    "            'Method': 'Revaluation',\n",
    "            'Queries': queries or r.Request.Queries,\n",
    "            'PreviousRequest': r.Request,\n",
    "            'PreviousResponse': r\n",
    "        }))\n",
    "\n",
    "    return ps.new_cm(big_request)\n",
    "\n",
    "\n",
    "def __calculate_pnl_category(request: msg.MessageList, prev_request: msg.MessageList, results: msg.MessageList, algo_id: str, category: str):\n",
    "    # Calculating request\n",
    "    calc_request = ps.compute_raw(algo_id, request).Result\n",
    "    calc_request = [x for x in calc_request if x.get('StatusText') is not None and x.StatusText == 'done']\n",
    "    calc_prev_request = ps.compute_raw(algo_id, prev_request).Result\n",
    "    calc_prev_request = [x for x in calc_prev_request if x.get('StatusText') is not None and x.StatusText == 'done']\n",
    "\n",
    "    # Matching results\n",
    "    for res in calc_request:\n",
    "        cpty = res.Request['_Counterparty']\n",
    "        result = next((x for x in results if x.Request['_Counterparty'] == cpty))\n",
    "        prev_res = next((x for x in calc_prev_request if x.Request['_Counterparty'] == cpty), None)\n",
    "        for query in res.Request.Queries:\n",
    "            result[query][category] = res.Result[query] - prev_res.Result[query] if prev_res else 0\n",
    "        if prev_res is not None:\n",
    "            calc_prev_request.remove(prev_res)\n",
    "\n",
    "    for res in calc_prev_request:\n",
    "        cpty = res.Request['_Counterparty']\n",
    "        result = next((x for x in results if x.Request['_Counterparty'] == cpty))\n",
    "        for query in res.Request.Queries:\n",
    "            result[query][category] = 0\n",
    "\n",
    "            \n",
    "def __update_marketDependencies(res_market_dependencies: msg.MessageList, results: msg.MessageList, all_market_dependencies: dict, excluded_dependencies: dict):\n",
    "\n",
    "    for res_dep in res_market_dependencies:\n",
    "        cpty = res_dep.Request['_Counterparty']\n",
    "        result = next(filter(lambda x: x.Request['_Counterparty'] == cpty, results))\n",
    "\n",
    "        for Type in res_dep.Result.MarketDependencies:\n",
    "            if Type == 'Fixings':\n",
    "                continue\n",
    "\n",
    "            for Identifier in res_dep.Result.MarketDependencies[Type]:\n",
    "                if excluded_dependencies.get(Type) and Identifier in excluded_dependencies.get(Type):\n",
    "                    continue\n",
    "\n",
    "                md_to = result.Request.Request.Model.MarketDataSet[Type][Identifier]\n",
    "                md_from = result.Request.PreviousRequest.Model.MarketDataSet[Type][Identifier]\n",
    "\n",
    "                if isinstance(md_to, float):\n",
    "                    if md_to != md_from:\n",
    "                        result.MarketDependencies.setdefault(Type, [])\n",
    "                        result.MarketDependencies[Type].append(Identifier)\n",
    "\n",
    "                        all_market_dependencies.setdefault(Type, [])\n",
    "                        if Identifier not in all_market_dependencies[Type]:\n",
    "                            all_market_dependencies[Type].append(Identifier)\n",
    "                elif md_to._id != md_from._id:\n",
    "                    result.MarketDependencies.setdefault(Type, [])\n",
    "                    result.MarketDependencies[Type].append(Identifier)\n",
    "\n",
    "                    all_market_dependencies.setdefault(Type, [])\n",
    "                    if Identifier not in all_market_dependencies[Type]:\n",
    "                        all_market_dependencies[Type].append(Identifier)\n",
    "\n",
    "\n",
    "def run_generic_revaluation_based_metric_change_explain_batch(big_request: msg.MessageList,\n",
    "                                                              algo_id: str = None,\n",
    "                                                              same_mds: bool = False,\n",
    "                                                              threshold: float = -1,\n",
    "                                                              optimize_credit_curves: bool = False) -> PsObject:\n",
    "    \"\"\"Generates revaluation based explanation report for the metrics requested.\n",
    "    PS! PnL Explain is calculated from the same Base (compared to other methods)\n",
    "    PPS! Make sure, input data is in correct format\n",
    "    PPPS! One request per one cpty in big request!\n",
    "\n",
    "    Args:\n",
    "        big_request (MessageList): A list of by-counterparty collected requests on two compared dates\n",
    "        algo_id (str): ID of the algo to use\n",
    "        same_mds (bool): True if within date and within prev_date the same mds is assigned (improves performance)\n",
    "        threshold: if Total change is less than threshold, then PnL Explain for given cpty is not calculated\n",
    "\n",
    "    Returns:\n",
    "        PsObject: revaluation based PnL explain summary\n",
    "    \"\"\"\n",
    "\n",
    "    if algo_id is None:\n",
    "        algo_id = env.global_algo_id()\n",
    "\n",
    "    results = ps.new_cm([])\n",
    "\n",
    "    env.logger.info('Calculating Total/Expired/New and removing failed cpties')\n",
    "\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    for req in big_request:\n",
    "        if req.get('Request'):\n",
    "            req.Request.Queries = req.Queries\n",
    "            req.Request.ReportingCurrency = req.ReportingCurrency\n",
    "            request.append(req.Request)\n",
    "        if req.get('PreviousRequest'):\n",
    "            req.PreviousRequest.Queries = req.Queries\n",
    "            req.PreviousRequest.ReportingCurrency = req.ReportingCurrency\n",
    "            prev_request.append(req.PreviousRequest)\n",
    "\n",
    "    ps.put(request)\n",
    "    ps.put(prev_request)\n",
    "\n",
    "    env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "    response = ps.compute_raw(algo_id, request)\n",
    "    prev_response = ps.compute_raw(algo_id, prev_request)\n",
    "\n",
    "    for req in big_request:\n",
    "        cpty = req['_Counterparty']\n",
    "        result = ps.new_cm({\n",
    "            '_Counterparty': cpty,\n",
    "            'Request': req,\n",
    "        })\n",
    "\n",
    "        if not req.get('Request'):\n",
    "            # Expired cpty\n",
    "            prev_res = next(filter(lambda x: x.Request['_Counterparty'] == cpty, prev_response.Result), None)\n",
    "            if prev_res.get('StatusText') and prev_res.get('StatusText') == 'done':\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': -req.PreviousResponse.Result[query],\n",
    "                        'ExpiredCounterparty': -req.PreviousResponse.Result[query]\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    query: ps.new_cm({'Total': 0}) for query in req.Queries\n",
    "                })\n",
    "            result['_NeedsToBeCalculated'] = False\n",
    "        elif not req.get('PreviousRequest'):\n",
    "            # New cpty\n",
    "            res = next(filter(lambda x: x.Request['_Counterparty'] == cpty, response.Result), None)\n",
    "            if res.get('StatusText') and res.get('StatusText') == 'done':\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': req.Response.Result[query],\n",
    "                        'NewCounterparty': req.Response.Result[query]\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    query: ps.new_cm({'Total': 0}) for query in req.Queries\n",
    "                })\n",
    "            result['_NeedsToBeCalculated'] = False\n",
    "        else:\n",
    "            res = next(filter(lambda x: x.Request['_Counterparty'] == cpty, response.Result), None)\n",
    "            prev_res = next(filter(lambda x: x.Request['_Counterparty'] == cpty, prev_response.Result), None)\n",
    "\n",
    "            if res.get('StatusText') and res.get('StatusText') == 'done' and prev_res.get('StatusText') and prev_res.get('StatusText') == 'done':\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': req.Response.Result[query] - req.PreviousResponse.Result[query]\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "                result['_NeedsToBeCalculated'] = any([abs(result[query]['Total']) > threshold for query in req.Queries])\n",
    "            elif res.get('StatusText') and res.get('StatusText') == 'done':\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': req.Response.Result[query],\n",
    "                        'ErrorPrevious': req.Response.Result[query]\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "                result['_NeedsToBeCalculated'] = False\n",
    "            elif prev_res.get('StatusText') and prev_res.get('StatusText') == 'done':\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': -req.PreviousResponse.Result[query],\n",
    "                        'ErrorCurrent': -req.PreviousResponse.Result[query]\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "                result['_NeedsToBeCalculated'] = False\n",
    "            else:\n",
    "                result.update({\n",
    "                    query: ps.new_cm({\n",
    "                        'Total': 0,\n",
    "                        'ErrorBoth': 0\n",
    "                    }) for query in req.Queries\n",
    "                })\n",
    "                result['_NeedsToBeCalculated'] = False\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    env.logger.info('Total calculated')\n",
    "\n",
    "    env.logger.info('Doing some preparations')\n",
    "\n",
    "    env.logger.info('Converting market data to canonical format')\n",
    "    # Converting market data to canonical format\n",
    "    if same_mds:\n",
    "        mds_to = market.to_canonical_format(next(filter(lambda x: x['_NeedsToBeCalculated'], results), None).Request.Request.Model.MarketDataSet, algo_id)\n",
    "        mds_from = market.to_canonical_format(next(filter(lambda x: x['_NeedsToBeCalculated'], results), None).Request.PreviousRequest.Model.MarketDataSet, algo_id)\n",
    "        for request in big_request:\n",
    "            if request.get('Request', None) is not None:\n",
    "                request.Request.Model.MarketDataSet = mds_to\n",
    "            if request.get('PreviousRequest', None) is not None:\n",
    "                request.PreviousRequest.Model.MarketDataSet = mds_from\n",
    "    else:\n",
    "        for result in results:\n",
    "            result.Request.Request.Model.MarketDataSet = market.to_canonical_format(result.Request.Request.Model.MarketDataSet, result.Request.Request.StaticDataSet, algo_id)\n",
    "            result.Request.PreviousRequest.Model.MarketDataSet = market.to_canonical_format(result.Request.PreviousRequest.Model.MarketDataSet, result.Request.PreviousRequest.StaticDataSet, algo_id)\n",
    "    env.logger.info('Converting done')\n",
    "\n",
    "    env.logger.info('Collecting dependencies')\n",
    "\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            res['MarketDependencies'] = {}\n",
    "            req = ps.get(ps.put(res.Request.Request))\n",
    "            req.Queries = ['MarketDependencies']\n",
    "            request.append(req)\n",
    "            req = ps.get(ps.put(res.Request.PreviousRequest))\n",
    "            req.Queries = ['MarketDependencies']\n",
    "            prev_request.append(req)\n",
    "\n",
    "    ps.put(request)\n",
    "    ps.put(prev_request)\n",
    "    env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "\n",
    "    res_market_dependencies = ps.compute_raw(algo_id, request).Result\n",
    "    res_prev_market_dependencies = ps.compute_raw(algo_id, prev_request).Result\n",
    "\n",
    "    all_market_dependencies = {}\n",
    "    excluded_dependencies = {\n",
    "        'RatesCurvesBundles': ['RUB_SOFR', 'USD_SOFR']\n",
    "    }\n",
    "    __update_marketDependencies(res_market_dependencies, results, all_market_dependencies, excluded_dependencies)\n",
    "    __update_marketDependencies(res_prev_market_dependencies, results, all_market_dependencies, excluded_dependencies)\n",
    "\n",
    "    del res_market_dependencies\n",
    "    del res_prev_market_dependencies\n",
    "\n",
    "    env.logger.info('Market dependencies collected')\n",
    "\n",
    "    env.logger.info('Preparations done')\n",
    "\n",
    "    # Calculating StaticDataSet change effect\n",
    "    env.logger.info('Calculating StaticDataSet change effect')\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            req = res.Request\n",
    "            if res['_NeedsToBeCalculated'] and req.Request.StaticDataSet._id != req.PreviousRequest.StaticDataSet._id:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                new_req.StaticDataSet = req.Request.StaticDataSet\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "    if needs_to_be_calculated:\n",
    "        __calculate_pnl_category(request, prev_request, results, algo_id, 'StaticDataSet')\n",
    "        env.logger.info('StaticDataSet change calculated')\n",
    "    else:\n",
    "        env.logger.info('StaticDataSet change not needed')\n",
    "\n",
    "    # Calculating Product change effect\n",
    "    env.logger.info('Calculating Product change effect')\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            req = res.Request\n",
    "            ns_from = ps.put(ps.new_cm(req.PreviousRequest.Product.NettingSets))\n",
    "            ns_to = ps.put(ps.new_cm(req.Request.Product.NettingSets))\n",
    "            if res['_NeedsToBeCalculated'] and ns_from._id != ns_to._id:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                new_req.Product.NettingSets = req.Request.Product.NettingSets\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "                continue\n",
    "    if needs_to_be_calculated:\n",
    "        __calculate_pnl_category(request, prev_request, results, algo_id, 'Product')\n",
    "        env.logger.info('Product change calculated')\n",
    "    else:\n",
    "        env.logger.info('Product change not needed')\n",
    "\n",
    "    # Check if cpties were switched from credit curve to internal rating or vice versa\n",
    "    env.logger.info('Credit model change')\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            req = res.Request\n",
    "            if 'CounterpartyCreditRating' in req.Request.Product and 'CounterpartyCreditRating' not in req.PreviousRequest.Product:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                del new_req.Product['CounterpartyCreditCurveIdentifier']\n",
    "                new_req.Product['CounterpartyCreditRating'] = req.Request.Product.CounterpartyCreditRating\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "            if 'CounterpartyCreditRating' not in req.Request.Product and 'CounterpartyCreditRating' in req.PreviousRequest.Product:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                del new_req.Product['CounterpartyCreditRating']\n",
    "                new_req.Product['CounterpartyCreditCurveIdentifier'] = req.Request.Product.OwnCreditCurveIdentifier\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "    if needs_to_be_calculated:\n",
    "        ps.put(request)\n",
    "        ps.put(prev_request)\n",
    "        env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "        __calculate_pnl_category(request, prev_request, results, algo_id, 'CreditModel')\n",
    "        env.logger.info('Credit model change calculated')\n",
    "    else:\n",
    "        env.logger.info('Credit model change not needed')\n",
    "\n",
    "    # Calculating Rating change effect\n",
    "    env.logger.info('Calculating Rating change effect')\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            req = res.Request\n",
    "            # If a cpty is calculated on its own curve, than we miss it\n",
    "            if 'CounterpartyCreditRating' not in req.Request.Product or 'CounterpartyCreditRating' not in req.PreviousRequest.Product:\n",
    "                continue\n",
    "            if req.Request.Product.CounterpartyCreditRating != req.PreviousRequest.Product.CounterpartyCreditRating:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                new_req.Product.CounterpartyCreditRating = req.Request.Product.CounterpartyCreditRating\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "    if needs_to_be_calculated:\n",
    "        __calculate_pnl_category(request, prev_request, results, algo_id, 'Rating')\n",
    "        env.logger.info('Rating change calculated')\n",
    "    else:\n",
    "        env.logger.info('Rating change not needed')\n",
    "\n",
    "    # Calculating LGD change effect\n",
    "    env.logger.info('Calculating LGD change effect')\n",
    "    request = ps.new_cm([])\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "    for res in results:\n",
    "        if res['_NeedsToBeCalculated']:\n",
    "            req = res.Request\n",
    "            if req.Request.Product.CounterpartyLossGivenDefault != req.PreviousRequest.Product.CounterpartyLossGivenDefault:\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                new_req.Product.CounterpartyLossGivenDefault = req.Request.Product.CounterpartyLossGivenDefault\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "    if needs_to_be_calculated:\n",
    "        __calculate_pnl_category(request, prev_request, results, algo_id, 'LGD')\n",
    "        env.logger.info('LGD change calculated')\n",
    "    else:\n",
    "        env.logger.info('LGD change not needed')\n",
    "\n",
    "   # Model change\n",
    "    env.logger.info('Model change effect calculation')\n",
    "    for model_key in ['NumeraireCurrency', 'TypeName']:\n",
    "\n",
    "        # First lets check model name and numeraire ccy change\n",
    "        env.logger.info(model_key + ' change effect calculation')\n",
    "        request = ps.new_cm([])\n",
    "        prev_request = ps.new_cm([])\n",
    "        needs_to_be_calculated = False\n",
    "        for res in results:\n",
    "            if res['_NeedsToBeCalculated']:\n",
    "                req = res.Request\n",
    "                if req.Request.Model[model_key] != req.PreviousRequest.Model[model_key]:\n",
    "                    prev_request.append(req.PreviousRequest)\n",
    "                    new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                    new_req.Model[model_key] = req.Request.Model[model_key]\n",
    "                    request.append(new_req)\n",
    "                    needs_to_be_calculated = True\n",
    "        if needs_to_be_calculated:\n",
    "            __calculate_pnl_category(request, prev_request, result, algo_id, 'Model.' + model_key)\n",
    "            env.logger.info(model_key + ' change effect done')\n",
    "        else:\n",
    "            env.logger.info(model_key + ' change not needed')\n",
    "\n",
    "    # Theta\n",
    "    env.logger.info('Theta effect calculation')\n",
    "    request = []\n",
    "    prev_request = ps.new_cm([])\n",
    "    needs_to_be_calculated = False\n",
    "\n",
    "    if same_mds:\n",
    "        fixings_from = ps.new_cm(mds_from.Fixings)\n",
    "        ps.put(fixings_from)\n",
    "        fixings_to = ps.new_cm(mds_to.Fixings)\n",
    "        ps.put(fixings_to)\n",
    "\n",
    "        if mds_to.AsOfDate != mds_from.AsOfDate or fixings_to._id != fixings_from._id:\n",
    "            request = ps.new_cm([x.Request.PreviousRequest for x in results if x['_NeedsToBeCalculated']])\n",
    "            request = ps.get(ps.put(request))\n",
    "            for req in request:\n",
    "                req.Model.MarketDataSet.AsOfDate = mds_to.AsOfDate\n",
    "                req.Model.MarketDataSet.Fixings = mds_to.Fixings\n",
    "            prev_request = ps.new_cm([x.Request.PreviousRequest for x in results if x['_NeedsToBeCalculated']])\n",
    "            ps.put(prev_request)\n",
    "            needs_to_be_calculated = True\n",
    "    else:\n",
    "        for res in results:\n",
    "            if res['_NeedsToBeCalculated']:\n",
    "                req = res.Request\n",
    "\n",
    "                fixings_from = ps.new_cm(req.PreviousRequest.Model.MarketDataSet.Fixings)\n",
    "                fixings_to = ps.new_cm(req.Request.Model.MarketDataSet.Fixings)\n",
    "                ps.put(fixings_from)\n",
    "                ps.put(fixings_to)\n",
    "\n",
    "                if req.Request.Model.MarketDataSet.AsOfDate != req.PreviousRequest.Model.MarketDataSet.AsOfDate\\\n",
    "                                                    or fixings_from._id != fixings_to._id:\n",
    "                    prev_request.append(req.PreviousRequest)\n",
    "                    new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                    new_req.Model.MarketDataSet.AsOfDate = req.Request.Model.MarketDataSet.AsOfDate\n",
    "                    request.append(new_req)\n",
    "                    needs_to_be_calculated = True\n",
    "        if needs_to_be_calculated:\n",
    "            request = ps.new_cm(request)\n",
    "            ps.put(request)\n",
    "            prev_request = ps.new_cm(prev_request)\n",
    "            ps.put(prev_request)\n",
    "    if needs_to_be_calculated:\n",
    "        env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "        __calculate_pnl_category(ps.new_cm(request), ps.new_cm(prev_request), results, algo_id, 'Model.MarketDataSet.AsOfDate')\n",
    "        env.logger.info('Theta effect done')\n",
    "    else:\n",
    "        env.logger.info('Theta not needed')\n",
    "\n",
    "    # Credit curves\n",
    "    # Done separately for optimization purposes\n",
    "    if optimize_credit_curves and all_market_dependencies.get('CreditCurves'):\n",
    "        env.logger.info('Processing CreditCurves')\n",
    "        request = []\n",
    "        prev_request = []\n",
    "        needs_to_be_calculated = False\n",
    "\n",
    "        env.logger.info('Collecting requests')\n",
    "        if same_mds:\n",
    "            md_to = next(filter(lambda x: x.Request.get('Request') is not None, results)).Request.Request.Model.MarketDataSet\n",
    "            md_from = next(filter(lambda x: x.Request.get('PreviousRequest') is not None, results)).Request.PreviousRequest.Model.MarketDataSet\n",
    "\n",
    "            # for curve in all_market_dependencies['CreditCurves']:\n",
    "            #     if md_to['CreditCurves'][curve]._id != md_from['CreditCurves'][curve]._id:\n",
    "            #         needs_to_be_calculated = True\n",
    "            #         break\n",
    "\n",
    "            new_md_to = ps.get(ps.put(md_from))\n",
    "            for curve in all_market_dependencies['CreditCurves']:\n",
    "                new_md_to.CreditCurves[curve] = md_to.CreditCurves[curve]\n",
    "\n",
    "            for res in results:\n",
    "                req = res.Request\n",
    "                if res['_NeedsToBeCalculated'] and 'CreditCurves' in res.MarketDependencies:\n",
    "                    prev_request.append(req.PreviousRequest)\n",
    "                    new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                    new_req.Model.MarketDataSet = new_md_to\n",
    "                    request.append(new_req)\n",
    "                    needs_to_be_calculated = True\n",
    "        else:\n",
    "            for res in results:\n",
    "                req = res.Request\n",
    "                if res['_NeedsToBeCalculated'] and 'CresitCurves' in res.MarketDependencies:\n",
    "                    md_to = req.Request.Model.MarketDataSet.CreditCurves\n",
    "                    md_from = req.PreviousRequest.Model.MarketDataSet.CreditCurves\n",
    "\n",
    "                    if md_to._id == md_from._id:\n",
    "                        continue\n",
    "\n",
    "                prev_request.append(req.PreviousRequest)\n",
    "                new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                new_req.Model.MarketDataSet.CreditCurves = md_to\n",
    "                request.append(new_req)\n",
    "                needs_to_be_calculated = True\n",
    "\n",
    "        if needs_to_be_calculated:\n",
    "            request = ps.new_cm(request)\n",
    "            ps.put(request)\n",
    "            prev_request = ps.new_cm(prev_request)\n",
    "            ps.put(prev_request)\n",
    "            env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "\n",
    "            # Calculating and removing unsuccessful results\n",
    "            calc_request = ps.compute_raw(algo_id, request).Result\n",
    "            calc_request = [x for x in calc_request if x.get('StatusText') is not None and x.StatusText == 'done']\n",
    "            calc_prev_request = ps.compute_raw(algo_id, prev_request).Result\n",
    "            calc_prev_request = [x for x in calc_prev_request if x.get('StatusText') is not None and x.StatusText == 'done']\n",
    "\n",
    "            # Matching results\n",
    "            for res in calc_request:\n",
    "                cpty = res.Request['_Counterparty']\n",
    "                result = next((x for x in results if x.Request['_Counterparty'] == cpty))\n",
    "                prev_res = next((x for x in calc_prev_request if x.Request['_Counterparty'] == cpty), None)\n",
    "\n",
    "                cva_change = res.Result['CVA'] - prev_res.Result['CVA'] if prev_res else None\n",
    "                cva_curve = res.Request.Product.get('CounterpartyCreditCurveIdentifier') or \\\n",
    "                            res.Request.StaticDataSet.CreditRatingToSpreadMapping[str(res.Request.Product.CounterpartyCreditRating)].BaseCurveIdentifier\n",
    "                dva_change = res.Result['DVA'] - prev_res.Result['DVA'] if prev_res else None\n",
    "                dva_curve = res.Request.Product.get('OwnCreditCurveIdentifier')\n",
    "\n",
    "                if 'ProductValue' in res.Request.Queries:\n",
    "                    result['ProductValue'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = 0\n",
    "                    result['ProductValue'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "                if 'CVA' in res.Request.Queries:\n",
    "                    result['CVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = cva_change\n",
    "                    result['CVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "                if 'DVA' in res.Request.Queries:\n",
    "                    result['DVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = 0\n",
    "                    result['DVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = dva_change\n",
    "                if 'BCVA' in res.Request.Queries:\n",
    "                    result['BCVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = cva_change\n",
    "                    if f'Model.MarketDataSet.CreditCurves.{dva_curve}' not in result['BCVA']:\n",
    "                        result['BCVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "                    result['BCVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] += dva_change\n",
    "\n",
    "                if prev_res:\n",
    "                    calc_prev_request.remove(prev_res)\n",
    "\n",
    "            for res in calc_prev_request:\n",
    "                cpty = res.Request['_Counterparty']\n",
    "                result = next((x for x in results if x.Request['_Counterparty'] == cpty))\n",
    "\n",
    "                cva_curve = res.Request.Product.get('CounterpartyCreditCurveIdentifier') or \\\n",
    "                            res.Request.StaticDataSet.CreditRatingToSpreadMapping[str(res.Request.Product.CounterpartyCreditRating)].BaseCurveIdentifier\n",
    "                dva_curve = res.Request.Product.get('OwnCreditCurveIdentifier')\n",
    "\n",
    "                result['CVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = 0\n",
    "                result['CVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "                result['DVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = 0\n",
    "                result['DVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "                result['BCVA'][f'Model.MarketDataSet.CreditCurves.{cva_curve}'] = 0\n",
    "                result['BCVA'][f'Model.MarketDataSet.CreditCurves.{dva_curve}'] = 0\n",
    "\n",
    "\n",
    "        all_market_dependencies.pop('CreditCurves')\n",
    "\n",
    "    # Other market data\n",
    "    for Type in all_market_dependencies:\n",
    "        for Identifier in all_market_dependencies[Type]:\n",
    "            env.logger.info(f'Processing {Type}.{Identifier}')\n",
    "            request = []\n",
    "            prev_request = []\n",
    "            needs_to_be_calculated = False\n",
    "\n",
    "            env.logger.info('Collecting requests')\n",
    "            if same_mds:\n",
    "\n",
    "                md_to = next(filter(lambda x: x.Request.get('Request') is not None, results)).Request.Request.Model.MarketDataSet\n",
    "                md_from = next(filter(lambda x: x.Request.get('PreviousRequest') is not None, results)).Request.PreviousRequest.Model.MarketDataSet\n",
    "\n",
    "                if isinstance(md_to[Type][Identifier], float):\n",
    "                    if md_to[Type][Identifier] == md_from[Type][Identifier]:\n",
    "                        continue\n",
    "                elif md_to[Type][Identifier]._id == md_from[Type][Identifier]._id:\n",
    "                    continue\n",
    "\n",
    "                new_md_to = ps.get(ps.put(md_from))\n",
    "                new_md_to[Type][Identifier] = md_to[Type][Identifier]\n",
    "\n",
    "                for res in results:\n",
    "                    req = res.Request\n",
    "                    if res['_NeedsToBeCalculated'] and Type in res.MarketDependencies \\\n",
    "                            and Identifier in res.MarketDependencies[Type]:\n",
    "\n",
    "                        prev_request.append(req.PreviousRequest)\n",
    "                        new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                        new_req.Model.MarketDataSet = new_md_to\n",
    "                        request.append(new_req)\n",
    "                        needs_to_be_calculated = True\n",
    "            else:\n",
    "                for res in results:\n",
    "                    req = res.Request\n",
    "                    if res['_NeedsToBeCalculated'] and Type in res.MarketDependencies \\\n",
    "                            and Identifier in res.MarketDependencies[Type]:\n",
    "\n",
    "                        md_to = req.Request.Model.MarketDataSet[Type][Identifier]\n",
    "                        md_from = req.PreviousRequest.Model.MarketDataSet[Type][Identifier]\n",
    "\n",
    "                        if isinstance(md_to, float):\n",
    "                            if md_to == md_from:\n",
    "                                continue\n",
    "                        elif md_to._id == md_from._id:\n",
    "                            continue\n",
    "\n",
    "                        prev_request.append(req.PreviousRequest)\n",
    "                        new_req = ps.get(ps.put(req.PreviousRequest))\n",
    "                        new_req.Model.MarketDataSet[Type][Identifier] = req.Request.Model.MarketDataSet[Type][Identifier]\n",
    "                        request.append(new_req)\n",
    "                        needs_to_be_calculated = True\n",
    "\n",
    "            if needs_to_be_calculated:\n",
    "                request = ps.new_cm(request)\n",
    "                ps.put(request)\n",
    "                prev_request = ps.new_cm(prev_request)\n",
    "                ps.put(prev_request)\n",
    "                env.logger.info(f'Requests collected. Request={request._id}. PrevRequest={prev_request._id}')\n",
    "                __calculate_pnl_category(request, prev_request, results, algo_id, 'Model.MarketDataSet.' + Type + '.' + Identifier)\n",
    "                env.logger.info(f'Processing {Type}.{Identifier} done')\n",
    "            else:\n",
    "                env.logger.info(f'Processing {Type}.{Identifier} not needed')\n",
    "\n",
    "    env.logger.info('Removing unnecessary info from result and adding Unexplained')\n",
    "    for res in results:\n",
    "        res.pop('MarketDependencies', None)\n",
    "        res.pop('_NeedsToBeCalculated', None)\n",
    "        res.Request.pop('Request', None)\n",
    "        res.Request.pop('PreviousRequest', None)\n",
    "        for query in res.Request.Queries:\n",
    "            res[query]['Unexplained'] = 2*res[query]['Total'] - sum(res[query].values())\n",
    "\n",
    "    env.logger.info('Unnecessary info deleted. Job is done')\n",
    "    return ps.new_cm(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f92df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-06-20 14:36:02,113 - Calculating Total/Expired/New and removing failed cpties\n",
      "[INFO] 2023-06-20 14:36:11,385 - Requests collected. Request=237e558492e6e4d7c57025159f76f141323ad7b2. PrevRequest=92af01b98aeb286d5d9791d8454532e4f237bb5e\n",
      "[INFO] 2023-06-20 14:36:40,423 - Total calculated\n",
      "[INFO] 2023-06-20 14:36:40,424 - Doing some preparations\n",
      "[INFO] 2023-06-20 14:36:40,425 - Converting market data to canonical format\n",
      "[INFO] 2023-06-20 14:36:40,980 - Converting done\n",
      "[INFO] 2023-06-20 14:36:40,981 - Collecting dependencies\n",
      "[INFO] 2023-06-20 14:39:29,903 - Requests collected. Request=4b3c9472c200e7b77177eb7bebcf1651b5143838. PrevRequest=e075d840a9cb47254748d33730d0f36944a9902c\n",
      "[INFO] 2023-06-20 14:41:47,733 - Market dependencies collected\n",
      "[INFO] 2023-06-20 14:41:47,734 - Preparations done\n",
      "[INFO] 2023-06-20 14:41:47,735 - Calculating StaticDataSet change effect\n",
      "[INFO] 2023-06-20 14:44:12,650 - StaticDataSet change calculated\n",
      "[INFO] 2023-06-20 14:44:12,652 - Calculating Product change effect\n",
      "[INFO] 2023-06-20 14:45:43,369 - Product change calculated\n",
      "[INFO] 2023-06-20 14:45:43,370 - Credit model change\n",
      "[INFO] 2023-06-20 14:45:43,438 - Credit model change not needed\n",
      "[INFO] 2023-06-20 14:45:43,440 - Calculating Rating change effect\n",
      "[INFO] 2023-06-20 14:45:43,461 - Rating change not needed\n",
      "[INFO] 2023-06-20 14:45:43,462 - Calculating LGD change effect\n",
      "[INFO] 2023-06-20 14:45:43,479 - LGD change not needed\n",
      "[INFO] 2023-06-20 14:45:43,482 - Model change effect calculation\n",
      "[INFO] 2023-06-20 14:45:43,483 - NumeraireCurrency change effect calculation\n",
      "[INFO] 2023-06-20 14:45:43,601 - NumeraireCurrency change not needed\n",
      "[INFO] 2023-06-20 14:45:43,603 - TypeName change effect calculation\n",
      "[INFO] 2023-06-20 14:45:43,619 - TypeName change not needed\n",
      "[INFO] 2023-06-20 14:45:43,621 - Theta effect calculation\n",
      "[INFO] 2023-06-20 14:46:18,799 - Requests collected. Request=fb05ccda6a6395fed999dcde5ca2facfc5e74410. PrevRequest=fb05ccda6a6395fed999dcde5ca2facfc5e74410\n",
      "[INFO] 2023-06-20 14:48:16,514 - Theta effect done\n",
      "[INFO] 2023-06-20 14:48:16,515 - Processing CreditCurves\n",
      "[INFO] 2023-06-20 14:48:16,520 - Collecting requests\n",
      "[INFO] 2023-06-20 14:49:46,984 - Requests collected. Request=d4a844e0781974c8f2d4576732c9acfedbd9d0d8. PrevRequest=13025cc3706e5d2f77a8693f55f499a791c0df7c\n",
      "[INFO] 2023-06-20 14:51:12,816 - Processing RatesCurvesBundles.RUB_RUONIA_OIS\n",
      "[INFO] 2023-06-20 14:51:12,818 - Collecting requests\n",
      "[INFO] 2023-06-20 14:52:40,888 - Requests collected. Request=a6ca561ccb8a57b6b80f688b1ddc32871e371cd5. PrevRequest=fb05ccda6a6395fed999dcde5ca2facfc5e74410\n",
      "[INFO] 2023-06-20 14:53:15,312 - Processing RatesCurvesBundles.RUB_RUONIA_OIS done\n",
      "[INFO] 2023-06-20 14:53:15,314 - Processing RatesCurvesBundles.RUB_KEY_RATE\n",
      "[INFO] 2023-06-20 14:53:15,315 - Collecting requests\n",
      "[INFO] 2023-06-20 14:54:37,939 - Requests collected. Request=092844ffa7c58303ebdb6c24e43c975807d9e77d. PrevRequest=fb05ccda6a6395fed999dcde5ca2facfc5e74410\n",
      "[INFO] 2023-06-20 14:55:12,754 - Processing RatesCurvesBundles.RUB_KEY_RATE done\n",
      "[INFO] 2023-06-20 14:55:12,756 - Processing RatesCurvesBundles.USD_LIBOR_3M\n",
      "[INFO] 2023-06-20 14:55:12,758 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:16,401 - Requests collected. Request=48c0df37c83eed4555a7453e8df60897dc6bd36e. PrevRequest=407be05f151f135dd5924b82412c52c15ce6a4db\n",
      "[INFO] 2023-06-20 14:55:17,880 - Processing RatesCurvesBundles.USD_LIBOR_3M done\n",
      "[INFO] 2023-06-20 14:55:17,881 - Processing RatesCurvesBundles.USD_XCCY\n",
      "[INFO] 2023-06-20 14:55:17,882 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:21,536 - Requests collected. Request=4df19c28d6c3339ea89ad6a4ec54e386434de944. PrevRequest=407be05f151f135dd5924b82412c52c15ce6a4db\n",
      "[INFO] 2023-06-20 14:55:23,038 - Processing RatesCurvesBundles.USD_XCCY done\n",
      "[INFO] 2023-06-20 14:55:23,039 - Processing RatesCurvesBundles.CNH_FX\n",
      "[INFO] 2023-06-20 14:55:23,040 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:26,357 - Requests collected. Request=dae9e1ee1631af1e92d455fb6761147b93a68000. PrevRequest=31eb21319a574bdc91a7f5fd3a343e91e6c109c7\n",
      "[INFO] 2023-06-20 14:55:27,795 - Processing RatesCurvesBundles.CNH_FX done\n",
      "[INFO] 2023-06-20 14:55:27,796 - Processing RatesCurvesBundles.CNH_SOFR\n",
      "[INFO] 2023-06-20 14:55:27,798 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:31,074 - Requests collected. Request=47f8c297925ca879ad981b0e5298f76e79551529. PrevRequest=6ead9ba1f871a6f31c46c6553a4b17fff4739560\n",
      "[INFO] 2023-06-20 14:55:32,375 - Processing RatesCurvesBundles.CNH_SOFR done\n",
      "[INFO] 2023-06-20 14:55:32,376 - Processing RatesCurvesBundles.EUR_EURIBOR_6M\n",
      "[INFO] 2023-06-20 14:55:32,378 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:34,602 - Requests collected. Request=b4eccd4965b916dfa2ef4dc21b4552d371ef4b82. PrevRequest=a28c0dda8ba30fd2178a592c0e283a1ab75abdae\n",
      "[INFO] 2023-06-20 14:55:35,494 - Processing RatesCurvesBundles.EUR_EURIBOR_6M done\n",
      "[INFO] 2023-06-20 14:55:35,496 - Processing RatesCurvesBundles.EUR_XCCY\n",
      "[INFO] 2023-06-20 14:55:35,497 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:37,696 - Requests collected. Request=3a29b397dea4312f5352511d2841808cd76ab65a. PrevRequest=a28c0dda8ba30fd2178a592c0e283a1ab75abdae\n",
      "[INFO] 2023-06-20 14:55:38,562 - Processing RatesCurvesBundles.EUR_XCCY done\n",
      "[INFO] 2023-06-20 14:55:38,564 - Processing RatesCurvesBundles.EUR_RUB\n",
      "[INFO] 2023-06-20 14:55:38,565 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:40,766 - Requests collected. Request=ff089f5a5d8e07fc7b8c78908b6f4730840f239d. PrevRequest=a28c0dda8ba30fd2178a592c0e283a1ab75abdae\n",
      "[INFO] 2023-06-20 14:55:41,697 - Processing RatesCurvesBundles.EUR_RUB done\n",
      "[INFO] 2023-06-20 14:55:41,698 - Processing RatesCurvesBundles.EUR_EURIBOR_3M\n",
      "[INFO] 2023-06-20 14:55:41,699 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:42,049 - Requests collected. Request=dcb6abba5081c7dd7b71977054aa04772ddd2cbb. PrevRequest=d78526924305a22823c4029649c83fe994e53b6b\n",
      "[INFO] 2023-06-20 14:55:42,183 - Processing RatesCurvesBundles.EUR_EURIBOR_3M done\n",
      "[INFO] 2023-06-20 14:55:42,184 - Processing RatesCurvesBundles.XAU_USD\n",
      "[INFO] 2023-06-20 14:55:42,185 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:42,779 - Requests collected. Request=853394bd7cd76e3cae3394a22aaf064b7247da92. PrevRequest=61049888ee64fcab07583f47e55f8a9f12c07cf3\n",
      "[INFO] 2023-06-20 14:55:43,041 - Processing RatesCurvesBundles.XAU_USD done\n",
      "[INFO] 2023-06-20 14:55:43,043 - Processing RatesCurvesBundles.RUB_MOSPRIME_3M\n",
      "[INFO] 2023-06-20 14:55:43,044 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:43,385 - Requests collected. Request=88d22fe2dcbf638082015a21229e89514706859c. PrevRequest=1bd7f237caafd60b4bd34040ab6833d1a189f7bb\n",
      "[INFO] 2023-06-20 14:55:43,526 - Processing RatesCurvesBundles.RUB_MOSPRIME_3M done\n",
      "[INFO] 2023-06-20 14:55:43,527 - Processing RatesCurvesBundles.JPY_XCCY\n",
      "[INFO] 2023-06-20 14:55:43,528 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:43,700 - Requests collected. Request=945b242bf42c76017a7c7b8ef7d738e9b863a593. PrevRequest=f09ae74450d6814c499e30044a97444cb0a13f88\n",
      "[INFO] 2023-06-20 14:55:43,742 - Processing RatesCurvesBundles.JPY_XCCY done\n",
      "[INFO] 2023-06-20 14:55:43,744 - Processing RatesCurvesBundles.USD_LIBOR_6M\n",
      "[INFO] 2023-06-20 14:55:43,746 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:43,891 - Requests collected. Request=0c44fc4e662c68b7962ae428eb2754e67bd358d2. PrevRequest=f9d89c23c8ebe0d2117102699f70334436e9cf06\n",
      "[INFO] 2023-06-20 14:55:43,938 - Processing RatesCurvesBundles.USD_LIBOR_6M done\n",
      "[INFO] 2023-06-20 14:55:43,940 - Processing RatesCurvesBundles.CHF_XCCY\n",
      "[INFO] 2023-06-20 14:55:43,941 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:44,116 - Requests collected. Request=8cd4e9bb9bd45ad8125d1ba3eb8232996bdfee44. PrevRequest=f5d64b1ad2b261f87be8d0daedf963f8ae5fa5ad\n",
      "[INFO] 2023-06-20 14:55:44,165 - Processing RatesCurvesBundles.CHF_XCCY done\n",
      "[INFO] 2023-06-20 14:55:44,166 - Processing RatesCurvesBundles.CHF_LIBOR_6M\n",
      "[INFO] 2023-06-20 14:55:44,168 - Collecting requests\n",
      "[INFO] 2023-06-20 14:55:44,314 - Requests collected. Request=c4d7a2794b48c2cc5416464019f0c24467300fd2. PrevRequest=f5d64b1ad2b261f87be8d0daedf963f8ae5fa5ad\n",
      "[INFO] 2023-06-20 14:55:44,358 - Processing RatesCurvesBundles.CHF_LIBOR_6M done\n",
      "[INFO] 2023-06-20 14:55:44,359 - Processing Spots.USDRUB\n",
      "[INFO] 2023-06-20 14:55:44,360 - Collecting requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2023-06-20 14:57:09,584 - Requests collected. Request=18d5479596485aee19d78fd5ee32ee1b44ad75d2. PrevRequest=fb05ccda6a6395fed999dcde5ca2facfc5e74410\n",
      "[INFO] 2023-06-20 14:57:44,124 - Processing Spots.USDRUB done\n",
      "[INFO] 2023-06-20 14:57:44,126 - Processing Spots.CNHRUB\n",
      "[INFO] 2023-06-20 14:57:44,127 - Collecting requests\n",
      "[INFO] 2023-06-20 14:57:47,590 - Requests collected. Request=05004d0206d9c60ff3aef888018c43b42b11a222. PrevRequest=31eb21319a574bdc91a7f5fd3a343e91e6c109c7\n",
      "[INFO] 2023-06-20 14:57:48,885 - Processing Spots.CNHRUB done\n",
      "[INFO] 2023-06-20 14:57:48,886 - Processing Spots.EURRUB\n",
      "[INFO] 2023-06-20 14:57:48,888 - Collecting requests\n",
      "[INFO] 2023-06-20 14:57:55,086 - Requests collected. Request=2003e73bbe2075ff46752b0856f4ebc86987efbb. PrevRequest=a28c0dda8ba30fd2178a592c0e283a1ab75abdae\n",
      "[INFO] 2023-06-20 14:57:55,987 - Processing Spots.EURRUB done\n",
      "[INFO] 2023-06-20 14:57:55,988 - Processing Spots.XAURUB\n",
      "[INFO] 2023-06-20 14:57:55,990 - Collecting requests\n",
      "[INFO] 2023-06-20 14:57:56,543 - Requests collected. Request=331a3d31812f9a57685bf6d3fd268ba38e204845. PrevRequest=61049888ee64fcab07583f47e55f8a9f12c07cf3\n",
      "[INFO] 2023-06-20 14:57:56,746 - Processing Spots.XAURUB done\n",
      "[INFO] 2023-06-20 14:57:56,747 - Processing Spots.JPYRUB\n",
      "[INFO] 2023-06-20 14:57:56,749 - Collecting requests\n",
      "[INFO] 2023-06-20 14:57:56,913 - Requests collected. Request=c625acd71dcb15f365468a2ffe52c2a1ae03ec10. PrevRequest=f09ae74450d6814c499e30044a97444cb0a13f88\n",
      "[INFO] 2023-06-20 14:57:56,960 - Processing Spots.JPYRUB done\n",
      "[INFO] 2023-06-20 14:57:56,962 - Processing Spots.CHFRUB\n",
      "[INFO] 2023-06-20 14:57:56,963 - Collecting requests\n",
      "[INFO] 2023-06-20 14:57:57,131 - Requests collected. Request=1e0629639e0cdb84f6797755394559c0c9830cf9. PrevRequest=f5d64b1ad2b261f87be8d0daedf963f8ae5fa5ad\n",
      "[INFO] 2023-06-20 14:57:57,177 - Processing Spots.CHFRUB done\n",
      "[INFO] 2023-06-20 14:57:57,179 - Processing VolatilitySurfaces.RUB_KEY_RATE\n",
      "[INFO] 2023-06-20 14:57:57,180 - Collecting requests\n",
      "[INFO] 2023-06-20 14:59:19,351 - Requests collected. Request=a9bf6d0b79e71f0db61d1dcbfcecf8ff45780a75. PrevRequest=fb05ccda6a6395fed999dcde5ca2facfc5e74410\n",
      "[INFO] 2023-06-20 14:59:53,135 - Processing VolatilitySurfaces.RUB_KEY_RATE done\n",
      "[INFO] 2023-06-20 14:59:53,136 - Processing VolatilitySurfaces.USD_LIBOR_3M\n",
      "[INFO] 2023-06-20 14:59:53,137 - Collecting requests\n",
      "[INFO] 2023-06-20 14:59:56,769 - Requests collected. Request=f684b9aa8bae716cf7f88aedb51766ac831c1d4b. PrevRequest=407be05f151f135dd5924b82412c52c15ce6a4db\n",
      "[INFO] 2023-06-20 14:59:58,224 - Processing VolatilitySurfaces.USD_LIBOR_3M done\n",
      "[INFO] 2023-06-20 14:59:58,225 - Processing VolatilitySurfaces.EUR_EURIBOR_6M\n",
      "[INFO] 2023-06-20 14:59:58,226 - Collecting requests\n",
      "[INFO] 2023-06-20 15:00:00,384 - Requests collected. Request=d8ff843ea903956057d46177301ccbbb1965f22c. PrevRequest=a28c0dda8ba30fd2178a592c0e283a1ab75abdae\n",
      "[INFO] 2023-06-20 15:00:01,254 - Processing VolatilitySurfaces.EUR_EURIBOR_6M done\n",
      "[INFO] 2023-06-20 15:00:01,255 - Removing unnecessary info from result and adding Unexplained\n",
      "[INFO] 2023-06-20 15:00:01,288 - Unnecessary info deleted. Job is done\n",
      "[INFO] 2023-06-20 15:00:01,351 - Saving results to Excel\n",
      "[INFO] 2023-06-20 15:01:25,799 - Results saved!\n"
     ]
    }
   ],
   "source": [
    "algo_id = '833aa9286a2951a0eac3fb38913d6990435e5c4c'\n",
    "prev_response = ps.new_cm(ps.get('b3908621858e2038b56eab7f0c6d8d787553c7f7').Result)\n",
    "response = ps.new_cm(ps.get('6bf53e7dd2480a587053c37eada8a67ddbc09715').Result)\n",
    "\n",
    "\n",
    "t_0 = prev_response[0].Request.Model.MarketDataSet.AsOfDate\n",
    "t_1 = response[0].Request.Model.MarketDataSet.AsOfDate\n",
    "\n",
    "queries = ['BCVA', 'CVA', 'DVA']\n",
    "big_request = prepare_big_request_for_pnl(response, prev_response,\n",
    "                                                   algo_id,\n",
    "                                                   'USD', queries)\n",
    "a = run_generic_revaluation_based_metric_change_explain_batch(big_request, algo_id, True, -1, True)\n",
    "\n",
    "\n",
    "ps_utils.env.logger.info('Saving results to Excel')\n",
    "df = pd.DataFrame(columns=['MetricType', 'Category', 'Counterparty', 'Value'])\n",
    "for result in a:\n",
    "    for metric in queries:\n",
    "        for category in result[metric]:\n",
    "            df = pd.concat([df, pd.DataFrame([{\n",
    "                'MetricType': metric,\n",
    "                'Category': category,\n",
    "                'Counterparty': result['_Counterparty'],\n",
    "                'Value': result[metric][category]}])])\n",
    "\n",
    "df_aggr = pd.DataFrame(df.groupby(['MetricType', 'Category'])['Value'].apply(sum)).pivot_table(index='Category', columns=['MetricType'], values='Value').fillna(0)\n",
    "\n",
    "writer = pd.ExcelWriter(f'PnL Explain_{t_0.strftime(\"%Y-%m-%d\")}_{t_1.strftime(\"%Y-%m-%d\")}.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='PnLExplain', index=False)\n",
    "df_aggr.to_excel(writer, sheet_name='PnLAggr', index=True)\n",
    "\n",
    "wb = writer.book\n",
    "ws1 = writer.sheets['PnLExplain']\n",
    "ws2 = writer.sheets['PnLAggr']\n",
    "nb_format = wb.add_format({'num_format': '#,##0'})\n",
    "ws1.set_column(0, 0, 15)\n",
    "ws1.set_column(1, 1, 60)\n",
    "ws1.set_column(2, 2, 15)\n",
    "ws1.set_column(3, 3, 10, nb_format)\n",
    "ws2.set_column(0, 0, 60)\n",
    "ws2.set_column(1, 3, 10, nb_format)\n",
    "\n",
    "writer.close()\n",
    "ps_utils.env.logger.info('Results saved!')\n",
    "\n",
    "#ps.put(a)\n",
    "#|ps_utils.env.logger.info(f'Result id is {a._id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ae7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
